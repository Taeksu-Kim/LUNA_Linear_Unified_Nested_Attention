{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo"
      ],
      "metadata": {
        "id": "FMcFF5vSKlHH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00abf857-e3bf-4d6a-cc8e-72fa11c7e659"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.7.1-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.7.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Taeksu-Kim/LUNA_Linear_Unified_Nested_Attention.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ImTWKdWvGley",
        "outputId": "6290b622-a48e-4bdb-853b-192862202985"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'LUNA_Linear_Unified_Nested_Attention'...\n",
            "remote: Enumerating objects: 94, done.\u001b[K\n",
            "remote: Counting objects: 100% (94/94), done.\u001b[K\n",
            "remote: Compressing objects: 100% (91/91), done.\u001b[K\n",
            "remote: Total 94 (delta 52), reused 7 (delta 2), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (94/94), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd LUNA_Linear_Unified_Nested_Attention"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fL7esnUGoYz",
        "outputId": "ffe56cc0-fa86-48ed-e95c-928c42c824bd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LUNA_Linear_Unified_Nested_Attention\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from LUNA import *\n",
        "from utils import Config"
      ],
      "metadata": {
        "id": "Q6xswPNuGpiT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKUYdi9_G427",
        "outputId": "359794d3-5705-4e86-a720-41f967eda2ca"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Taeksu-Kim/Transformer.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GU9wFNk_G5z0",
        "outputId": "0d87de5b-b96a-40cc-85d8-7149814fc3dd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Transformer'...\n",
            "remote: Enumerating objects: 88, done.\u001b[K\n",
            "remote: Counting objects: 100% (88/88), done.\u001b[K\n",
            "remote: Compressing objects: 100% (59/59), done.\u001b[K\n",
            "remote: Total 88 (delta 36), reused 41 (delta 15), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (88/88), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd  ./Transformer/PyTorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gg4x-jlGG7yd",
        "outputId": "72ab51a2-f0b6-4a68-ff0e-f75de9f937f4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Transformer/PyTorch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformer import TransformerEncoder, Transformer"
      ],
      "metadata": {
        "id": "V8SCB4ZtG8_e"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from torchinfo import summary"
      ],
      "metadata": {
        "id": "cI-T1cOaHWhD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 8\n",
        "embedding_dim =  768\n",
        "max_input_len = 512\n",
        "max_dec_len = 32\n",
        "num_layer = 12\n",
        "num_att_head = 12\n",
        "feed_forward_dim = 1024\n",
        "p_length = 128\n",
        "vocab_size = 32500\n",
        "dynamic_projection = False\n",
        "tie_key_value = False\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "L-wZhZRNHeeQ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_len = max_input_len\n",
        "\n",
        "inputs = []\n",
        "\n",
        "for i in range(batch_size):\n",
        "  input_len = random.randint(seq_len-20, seq_len-10)\n",
        "  pad_len = seq_len - input_len\n",
        "  line = []\n",
        "  for j in range(input_len):\n",
        "    line.append(random.randint(1, vocab_size-1))\n",
        "  line += [0] * pad_len\n",
        "  inputs.append(line)\n",
        "\n",
        "inputs = torch.tensor(inputs)"
      ],
      "metadata": {
        "id": "oZgf9fW8Hxer"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_labels = [0]*int(batch_size/2) + [1]*int(batch_size/2)\n",
        "encoder_labels = torch.tensor(encoder_labels)"
      ],
      "metadata": {
        "id": "xx3X8IXSqSkT"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_len = max_dec_len\n",
        "\n",
        "decoder_labels = []\n",
        "\n",
        "for i in range(batch_size):\n",
        "  input_len = random.randint(seq_len-20, seq_len-10)\n",
        "  pad_len = seq_len - input_len\n",
        "  line = []\n",
        "  for j in range(input_len):\n",
        "    line.append(random.randint(1, vocab_size-1))\n",
        "  line += [0] * pad_len\n",
        "  decoder_labels.append(line)\n",
        "\n",
        "decoder_labels = torch.tensor(decoder_labels)"
      ],
      "metadata": {
        "id": "JkQjkXLOHxhE"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs.shape, encoder_labels.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B48_I9vXHxjb",
        "outputId": "e2521109-dcea-4a41-c3bd-c4d43fa437ca"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([8, 512]), torch.Size([8]))"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config_dict = {\n",
        "'vocab_size': vocab_size,\n",
        "'d_model': embedding_dim,\n",
        "'max_enc_len': max_input_len,\n",
        "'max_dec_len': max_dec_len,\n",
        "'pad_id': 0,\n",
        "'use_decoder': True,\n",
        "'init_std': 0.02,\n",
        "'norm_eps': 1e-12,\n",
        "'drop_out_raito': 0.1,\n",
        "'num_enc_layers': num_layer,\n",
        "'num_dec_layers': num_layer,\n",
        "'num_att_heads': num_att_head,\n",
        "'feed_forward_dim': feed_forward_dim,\n",
        "'p_length': p_length,\n",
        "'p_drop_out_raito': 0.3,\n",
        "'dynamic_projection':dynamic_projection,\n",
        "'tie_key_value':tie_key_value,\n",
        "'decoder_only':False\n",
        "}\n",
        "\n",
        "config = Config(config_dict)"
      ],
      "metadata": {
        "id": "8E2khTJvHeg_"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "luna_encoder = Luna_TransformerEncoder(config)"
      ],
      "metadata": {
        "id": "0P1Q2T9hHejn"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vanillar_encoder = TransformerEncoder(config)"
      ],
      "metadata": {
        "id": "IDnZ206RHemQ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary(vanillar_encoder, input_data=[inputs])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZsmv0DrHesA",
        "outputId": "fa8ff052-76dd-41c8-9c87-26fd6c536bda"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==============================================================================================================\n",
              "Layer (type:depth-idx)                                       Output Shape              Param #\n",
              "==============================================================================================================\n",
              "TransformerEncoder                                           [8, 512, 768]             --\n",
              "├─Embedding: 1-1                                             [8, 512, 768]             24,960,000\n",
              "├─ModuleList: 1-2                                            --                        --\n",
              "│    └─TransformerEncoderLayer: 2-1                          [8, 512, 768]             --\n",
              "│    │    └─AddNorm: 3-1                                     [8, 512, 768]             2,363,904\n",
              "│    │    └─AddNorm: 3-2                                     [8, 512, 768]             1,576,192\n",
              "│    └─TransformerEncoderLayer: 2-2                          [8, 512, 768]             --\n",
              "│    │    └─AddNorm: 3-3                                     [8, 512, 768]             2,363,904\n",
              "│    │    └─AddNorm: 3-4                                     [8, 512, 768]             1,576,192\n",
              "│    └─TransformerEncoderLayer: 2-3                          [8, 512, 768]             --\n",
              "│    │    └─AddNorm: 3-5                                     [8, 512, 768]             2,363,904\n",
              "│    │    └─AddNorm: 3-6                                     [8, 512, 768]             1,576,192\n",
              "│    └─TransformerEncoderLayer: 2-4                          [8, 512, 768]             --\n",
              "│    │    └─AddNorm: 3-7                                     [8, 512, 768]             2,363,904\n",
              "│    │    └─AddNorm: 3-8                                     [8, 512, 768]             1,576,192\n",
              "│    └─TransformerEncoderLayer: 2-5                          [8, 512, 768]             --\n",
              "│    │    └─AddNorm: 3-9                                     [8, 512, 768]             2,363,904\n",
              "│    │    └─AddNorm: 3-10                                    [8, 512, 768]             1,576,192\n",
              "│    └─TransformerEncoderLayer: 2-6                          [8, 512, 768]             --\n",
              "│    │    └─AddNorm: 3-11                                    [8, 512, 768]             2,363,904\n",
              "│    │    └─AddNorm: 3-12                                    [8, 512, 768]             1,576,192\n",
              "│    └─TransformerEncoderLayer: 2-7                          [8, 512, 768]             --\n",
              "│    │    └─AddNorm: 3-13                                    [8, 512, 768]             2,363,904\n",
              "│    │    └─AddNorm: 3-14                                    [8, 512, 768]             1,576,192\n",
              "│    └─TransformerEncoderLayer: 2-8                          [8, 512, 768]             --\n",
              "│    │    └─AddNorm: 3-15                                    [8, 512, 768]             2,363,904\n",
              "│    │    └─AddNorm: 3-16                                    [8, 512, 768]             1,576,192\n",
              "│    └─TransformerEncoderLayer: 2-9                          [8, 512, 768]             --\n",
              "│    │    └─AddNorm: 3-17                                    [8, 512, 768]             2,363,904\n",
              "│    │    └─AddNorm: 3-18                                    [8, 512, 768]             1,576,192\n",
              "│    └─TransformerEncoderLayer: 2-10                         [8, 512, 768]             --\n",
              "│    │    └─AddNorm: 3-19                                    [8, 512, 768]             2,363,904\n",
              "│    │    └─AddNorm: 3-20                                    [8, 512, 768]             1,576,192\n",
              "│    └─TransformerEncoderLayer: 2-11                         [8, 512, 768]             --\n",
              "│    │    └─AddNorm: 3-21                                    [8, 512, 768]             2,363,904\n",
              "│    │    └─AddNorm: 3-22                                    [8, 512, 768]             1,576,192\n",
              "│    └─TransformerEncoderLayer: 2-12                         [8, 512, 768]             --\n",
              "│    │    └─AddNorm: 3-23                                    [8, 512, 768]             2,363,904\n",
              "│    │    └─AddNorm: 3-24                                    [8, 512, 768]             1,576,192\n",
              "==============================================================================================================\n",
              "Total params: 72,241,152\n",
              "Trainable params: 72,241,152\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 577.93\n",
              "==============================================================================================================\n",
              "Input size (MB): 0.03\n",
              "Forward/backward pass size (MB): 2541.75\n",
              "Params size (MB): 288.96\n",
              "Estimated Total Size (MB): 2830.75\n",
              "=============================================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary(luna_encoder, input_data=[inputs])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7EU5GPNHeuf",
        "outputId": "bbb0c286-e3a5-489d-cb6f-fbc76c46c48a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==============================================================================================================\n",
              "Layer (type:depth-idx)                                       Output Shape              Param #\n",
              "==============================================================================================================\n",
              "Luna_TransformerEncoder                                      [8, 512, 768]             98,304\n",
              "├─Embedding: 1-1                                             [8, 512, 768]             24,960,000\n",
              "├─Dropout: 1-2                                               [8, 512, 768]             --\n",
              "├─Dropout: 1-3                                               [8, 128, 768]             --\n",
              "├─ModuleList: 1-4                                            --                        --\n",
              "│    └─LunaTransformerEncoderLayer: 2-1                      [8, 512, 768]             --\n",
              "│    │    └─LinearUnifiedNestedAttention: 3-1                [8, 128, 768]             4,134,144\n",
              "│    │    └─LayerNorm: 3-2                                   [8, 128, 768]             1,536\n",
              "│    │    └─LayerNorm: 3-3                                   [8, 512, 768]             1,536\n",
              "│    │    └─PoswiseFeedForward: 3-4                          [8, 512, 768]             1,574,656\n",
              "│    │    └─LayerNorm: 3-5                                   [8, 512, 768]             1,536\n",
              "│    └─LunaTransformerEncoderLayer: 2-2                      [8, 512, 768]             --\n",
              "│    │    └─LinearUnifiedNestedAttention: 3-6                [8, 128, 768]             4,134,144\n",
              "│    │    └─LayerNorm: 3-7                                   [8, 128, 768]             1,536\n",
              "│    │    └─LayerNorm: 3-8                                   [8, 512, 768]             1,536\n",
              "│    │    └─PoswiseFeedForward: 3-9                          [8, 512, 768]             1,574,656\n",
              "│    │    └─LayerNorm: 3-10                                  [8, 512, 768]             1,536\n",
              "│    └─LunaTransformerEncoderLayer: 2-3                      [8, 512, 768]             --\n",
              "│    │    └─LinearUnifiedNestedAttention: 3-11               [8, 128, 768]             4,134,144\n",
              "│    │    └─LayerNorm: 3-12                                  [8, 128, 768]             1,536\n",
              "│    │    └─LayerNorm: 3-13                                  [8, 512, 768]             1,536\n",
              "│    │    └─PoswiseFeedForward: 3-14                         [8, 512, 768]             1,574,656\n",
              "│    │    └─LayerNorm: 3-15                                  [8, 512, 768]             1,536\n",
              "│    └─LunaTransformerEncoderLayer: 2-4                      [8, 512, 768]             --\n",
              "│    │    └─LinearUnifiedNestedAttention: 3-16               [8, 128, 768]             4,134,144\n",
              "│    │    └─LayerNorm: 3-17                                  [8, 128, 768]             1,536\n",
              "│    │    └─LayerNorm: 3-18                                  [8, 512, 768]             1,536\n",
              "│    │    └─PoswiseFeedForward: 3-19                         [8, 512, 768]             1,574,656\n",
              "│    │    └─LayerNorm: 3-20                                  [8, 512, 768]             1,536\n",
              "│    └─LunaTransformerEncoderLayer: 2-5                      [8, 512, 768]             --\n",
              "│    │    └─LinearUnifiedNestedAttention: 3-21               [8, 128, 768]             4,134,144\n",
              "│    │    └─LayerNorm: 3-22                                  [8, 128, 768]             1,536\n",
              "│    │    └─LayerNorm: 3-23                                  [8, 512, 768]             1,536\n",
              "│    │    └─PoswiseFeedForward: 3-24                         [8, 512, 768]             1,574,656\n",
              "│    │    └─LayerNorm: 3-25                                  [8, 512, 768]             1,536\n",
              "│    └─LunaTransformerEncoderLayer: 2-6                      [8, 512, 768]             --\n",
              "│    │    └─LinearUnifiedNestedAttention: 3-26               [8, 128, 768]             4,134,144\n",
              "│    │    └─LayerNorm: 3-27                                  [8, 128, 768]             1,536\n",
              "│    │    └─LayerNorm: 3-28                                  [8, 512, 768]             1,536\n",
              "│    │    └─PoswiseFeedForward: 3-29                         [8, 512, 768]             1,574,656\n",
              "│    │    └─LayerNorm: 3-30                                  [8, 512, 768]             1,536\n",
              "│    └─LunaTransformerEncoderLayer: 2-7                      [8, 512, 768]             --\n",
              "│    │    └─LinearUnifiedNestedAttention: 3-31               [8, 128, 768]             4,134,144\n",
              "│    │    └─LayerNorm: 3-32                                  [8, 128, 768]             1,536\n",
              "│    │    └─LayerNorm: 3-33                                  [8, 512, 768]             1,536\n",
              "│    │    └─PoswiseFeedForward: 3-34                         [8, 512, 768]             1,574,656\n",
              "│    │    └─LayerNorm: 3-35                                  [8, 512, 768]             1,536\n",
              "│    └─LunaTransformerEncoderLayer: 2-8                      [8, 512, 768]             --\n",
              "│    │    └─LinearUnifiedNestedAttention: 3-36               [8, 128, 768]             4,134,144\n",
              "│    │    └─LayerNorm: 3-37                                  [8, 128, 768]             1,536\n",
              "│    │    └─LayerNorm: 3-38                                  [8, 512, 768]             1,536\n",
              "│    │    └─PoswiseFeedForward: 3-39                         [8, 512, 768]             1,574,656\n",
              "│    │    └─LayerNorm: 3-40                                  [8, 512, 768]             1,536\n",
              "│    └─LunaTransformerEncoderLayer: 2-9                      [8, 512, 768]             --\n",
              "│    │    └─LinearUnifiedNestedAttention: 3-41               [8, 128, 768]             4,134,144\n",
              "│    │    └─LayerNorm: 3-42                                  [8, 128, 768]             1,536\n",
              "│    │    └─LayerNorm: 3-43                                  [8, 512, 768]             1,536\n",
              "│    │    └─PoswiseFeedForward: 3-44                         [8, 512, 768]             1,574,656\n",
              "│    │    └─LayerNorm: 3-45                                  [8, 512, 768]             1,536\n",
              "│    └─LunaTransformerEncoderLayer: 2-10                     [8, 512, 768]             --\n",
              "│    │    └─LinearUnifiedNestedAttention: 3-46               [8, 128, 768]             4,134,144\n",
              "│    │    └─LayerNorm: 3-47                                  [8, 128, 768]             1,536\n",
              "│    │    └─LayerNorm: 3-48                                  [8, 512, 768]             1,536\n",
              "│    │    └─PoswiseFeedForward: 3-49                         [8, 512, 768]             1,574,656\n",
              "│    │    └─LayerNorm: 3-50                                  [8, 512, 768]             1,536\n",
              "│    └─LunaTransformerEncoderLayer: 2-11                     [8, 512, 768]             --\n",
              "│    │    └─LinearUnifiedNestedAttention: 3-51               [8, 128, 768]             4,134,144\n",
              "│    │    └─LayerNorm: 3-52                                  [8, 128, 768]             1,536\n",
              "│    │    └─LayerNorm: 3-53                                  [8, 512, 768]             1,536\n",
              "│    │    └─PoswiseFeedForward: 3-54                         [8, 512, 768]             1,574,656\n",
              "│    │    └─LayerNorm: 3-55                                  [8, 512, 768]             1,536\n",
              "│    └─LunaTransformerEncoderLayer: 2-12                     [8, 512, 768]             --\n",
              "│    │    └─LinearUnifiedNestedAttention: 3-56               [8, 128, 768]             4,134,144\n",
              "│    │    └─LayerNorm: 3-57                                  [8, 128, 768]             1,536\n",
              "│    │    └─LayerNorm: 3-58                                  [8, 512, 768]             1,536\n",
              "│    │    └─PoswiseFeedForward: 3-59                         [8, 512, 768]             1,574,656\n",
              "│    │    └─LayerNorm: 3-60                                  [8, 512, 768]             1,536\n",
              "==============================================================================================================\n",
              "Total params: 93,619,200\n",
              "Trainable params: 93,619,200\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 748.17\n",
              "==============================================================================================================\n",
              "Input size (MB): 0.03\n",
              "Forward/backward pass size (MB): 2843.74\n",
              "Params size (MB): 374.08\n",
              "Estimated Total Size (MB): 3217.85\n",
              "=============================================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class simple_classfier(nn.Module):\n",
        "    def __init__(self, encoder, config):\n",
        "      super(simple_classfier, self).__init__()\n",
        "      self.encoder = encoder\n",
        "      self.fc = nn.Linear(config.d_model,2)\n",
        "\n",
        "    def forward(self, inputs, labels=None):\n",
        "      logits = self.encoder(inputs)[0]\n",
        "      logits = self.fc(logits[:,0])\n",
        "\n",
        "      return (logits,)"
      ],
      "metadata": {
        "id": "O_tGoKeNTowL"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vanillar_encoder_classifier = simple_classfier(vanillar_encoder, config)"
      ],
      "metadata": {
        "id": "eGdGk71gTo1L"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "luna_encoder_classifier = simple_classfier(luna_encoder, config)"
      ],
      "metadata": {
        "id": "iYmxux3aTo3r"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, inputs, labels):\n",
        "  model.to(device)\n",
        "  inputs = inputs.to(device)\n",
        "  labels = labels.to(device)\n",
        "\n",
        "  optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
        "\n",
        "  for epoch in tqdm(range(iter_steps)):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    with torch.cuda.amp.autocast():\n",
        "        logits = model(inputs,\n",
        "                       labels)[0]\n",
        "\n",
        "        loss_fnc = nn.CrossEntropyLoss()\n",
        "        if len(logits.size()) == 2:\n",
        "          loss = loss_fnc(logits, labels)\n",
        "        else:\n",
        "          num_class = logits.size()[-1]\n",
        "          loss = loss_fnc(logits.view(-1, num_class), labels.view(-1))\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "pO7qb-I7TpCE"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iter_steps = 300"
      ],
      "metadata": {
        "id": "5YnKIHtHavRp"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(vanillar_encoder_classifier, inputs, encoder_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WEH4gutZWZzQ",
        "outputId": "798ecc6a-99be-412b-db74-c051428d6791"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [01:06<00:00,  4.49it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train(luna_encoder_classifier, inputs, encoder_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8dTqBXTWZ4P",
        "outputId": "0748a4fc-b8d5-4581-d4a0-51426ed0f3cf"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [00:59<00:00,  5.04it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 8\n",
        "embedding_dim = 768\n",
        "max_input_len = 512\n",
        "max_dec_len = 128\n",
        "num_layer = 12\n",
        "num_att_head = 12\n",
        "feed_forward_dim = 1024\n",
        "p_length = 32\n",
        "vocab_size = 32500\n",
        "dynamic_projection = False\n",
        "tie_key_value = False\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "dG35XwkVqaAv"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config_dict = {\n",
        "'vocab_size': vocab_size,\n",
        "'d_model': embedding_dim,\n",
        "'max_enc_len': max_input_len,\n",
        "'max_dec_len': max_dec_len,\n",
        "'pad_id': 0,\n",
        "'use_decoder': True,\n",
        "'init_std': 0.02,\n",
        "'norm_eps': 1e-12,\n",
        "'drop_out_raito': 0.1,\n",
        "'num_enc_layers': num_layer,\n",
        "'num_dec_layers': num_layer,\n",
        "'num_att_heads': num_att_head,\n",
        "'feed_forward_dim': feed_forward_dim,\n",
        "'p_length': p_length,\n",
        "'p_drop_out_raito': 0.3,\n",
        "'dynamic_projection':dynamic_projection,\n",
        "'tie_key_value':tie_key_value,\n",
        "'decoder_only':False\n",
        "}\n",
        "\n",
        "config = Config(config_dict)"
      ],
      "metadata": {
        "id": "XJQCDE_ErAJy"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_len = max_input_len\n",
        "\n",
        "inputs = []\n",
        "\n",
        "for i in range(batch_size):\n",
        "  input_len = random.randint(seq_len-20, seq_len-10)\n",
        "  pad_len = seq_len - input_len\n",
        "  line = []\n",
        "  for j in range(input_len):\n",
        "    line.append(random.randint(1, vocab_size-1))\n",
        "  line += [0] * pad_len\n",
        "  inputs.append(line)\n",
        "\n",
        "inputs = torch.tensor(inputs)"
      ],
      "metadata": {
        "id": "i4S4sNa_qaAv"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_len = max_dec_len\n",
        "\n",
        "decoder_labels = []\n",
        "\n",
        "for i in range(batch_size):\n",
        "  input_len = random.randint(seq_len-20, seq_len-10)\n",
        "  pad_len = seq_len - input_len\n",
        "  line = []\n",
        "  for j in range(input_len):\n",
        "    line.append(random.randint(1, vocab_size-1))\n",
        "  line += [0] * pad_len\n",
        "  decoder_labels.append(line)\n",
        "\n",
        "decoder_labels = torch.tensor(decoder_labels)"
      ],
      "metadata": {
        "id": "waT2vETsqaAw"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs.shape, decoder_labels.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49b7c019-e995-47f4-a69a-c02ec3c0aeda",
        "id": "ZzUqm-f_qaAw"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([8, 512]), torch.Size([8, 128]))"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "luna_transformer = Luna_Transformer(config)"
      ],
      "metadata": {
        "id": "SNxqMyjGJojf"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vanillar_transformer = Transformer(config)"
      ],
      "metadata": {
        "id": "4xKVhI0eMtU2"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary(vanillar_transformer, input_data=[inputs,decoder_labels])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9DbOSxlMyd_",
        "outputId": "2ffb93e6-f336-42ce-c97d-fe60c509ca6b"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "===================================================================================================================\n",
              "Layer (type:depth-idx)                                            Output Shape              Param #\n",
              "===================================================================================================================\n",
              "Transformer                                                       [8, 128, 32500]           --\n",
              "├─TransformerEncoder: 1-1                                         [8, 512, 768]             --\n",
              "│    └─Embedding: 2-1                                             [8, 512, 768]             24,960,000\n",
              "│    └─ModuleList: 2-2                                            --                        --\n",
              "│    │    └─TransformerEncoderLayer: 3-1                          [8, 512, 768]             3,940,096\n",
              "│    │    └─TransformerEncoderLayer: 3-2                          [8, 512, 768]             3,940,096\n",
              "│    │    └─TransformerEncoderLayer: 3-3                          [8, 512, 768]             3,940,096\n",
              "│    │    └─TransformerEncoderLayer: 3-4                          [8, 512, 768]             3,940,096\n",
              "│    │    └─TransformerEncoderLayer: 3-5                          [8, 512, 768]             3,940,096\n",
              "│    │    └─TransformerEncoderLayer: 3-6                          [8, 512, 768]             3,940,096\n",
              "│    │    └─TransformerEncoderLayer: 3-7                          [8, 512, 768]             3,940,096\n",
              "│    │    └─TransformerEncoderLayer: 3-8                          [8, 512, 768]             3,940,096\n",
              "│    │    └─TransformerEncoderLayer: 3-9                          [8, 512, 768]             3,940,096\n",
              "│    │    └─TransformerEncoderLayer: 3-10                         [8, 512, 768]             3,940,096\n",
              "│    │    └─TransformerEncoderLayer: 3-11                         [8, 512, 768]             3,940,096\n",
              "│    │    └─TransformerEncoderLayer: 3-12                         [8, 512, 768]             3,940,096\n",
              "├─TransformerDecoder: 1-2                                         [8, 128, 32500]           --\n",
              "│    └─Embedding: 2-3                                             [8, 128, 768]             24,960,000\n",
              "│    └─ModuleList: 2-4                                            --                        --\n",
              "│    │    └─TransformerDecoderLayer: 3-13                         [8, 128, 768]             6,304,000\n",
              "│    │    └─TransformerDecoderLayer: 3-14                         [8, 128, 768]             6,304,000\n",
              "│    │    └─TransformerDecoderLayer: 3-15                         [8, 128, 768]             6,304,000\n",
              "│    │    └─TransformerDecoderLayer: 3-16                         [8, 128, 768]             6,304,000\n",
              "│    │    └─TransformerDecoderLayer: 3-17                         [8, 128, 768]             6,304,000\n",
              "│    │    └─TransformerDecoderLayer: 3-18                         [8, 128, 768]             6,304,000\n",
              "│    │    └─TransformerDecoderLayer: 3-19                         [8, 128, 768]             6,304,000\n",
              "│    │    └─TransformerDecoderLayer: 3-20                         [8, 128, 768]             6,304,000\n",
              "│    │    └─TransformerDecoderLayer: 3-21                         [8, 128, 768]             6,304,000\n",
              "│    │    └─TransformerDecoderLayer: 3-22                         [8, 128, 768]             6,304,000\n",
              "│    │    └─TransformerDecoderLayer: 3-23                         [8, 128, 768]             6,304,000\n",
              "│    │    └─TransformerDecoderLayer: 3-24                         [8, 128, 768]             6,304,000\n",
              "│    └─Linear: 2-5                                                [8, 128, 32500]           24,992,500\n",
              "===================================================================================================================\n",
              "Total params: 197,841,652\n",
              "Trainable params: 197,841,652\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (G): 1.58\n",
              "===================================================================================================================\n",
              "Input size (MB): 0.04\n",
              "Forward/backward pass size (MB): 4273.90\n",
              "Params size (MB): 791.37\n",
              "Estimated Total Size (MB): 5065.31\n",
              "==================================================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary(luna_transformer, input_data=[inputs,decoder_labels])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ie1Z4djSM0OS",
        "outputId": "7c8d97c6-3493-4202-8ae0-86fd94be1de7"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "===================================================================================================================\n",
              "Layer (type:depth-idx)                                            Output Shape              Param #\n",
              "===================================================================================================================\n",
              "Luna_Transformer                                                  [8, 128, 32500]           --\n",
              "├─Luna_TransformerEncoder: 1-1                                    [8, 512, 768]             24,576\n",
              "│    └─Embedding: 2-1                                             [8, 512, 768]             24,960,000\n",
              "│    └─Dropout: 2-2                                               [8, 512, 768]             --\n",
              "│    └─Dropout: 2-3                                               [8, 32, 768]              --\n",
              "│    └─ModuleList: 2-4                                            --                        --\n",
              "│    │    └─LunaTransformerEncoderLayer: 3-1                      [8, 512, 768]             5,713,408\n",
              "│    │    └─LunaTransformerEncoderLayer: 3-2                      [8, 512, 768]             5,713,408\n",
              "│    │    └─LunaTransformerEncoderLayer: 3-3                      [8, 512, 768]             5,713,408\n",
              "│    │    └─LunaTransformerEncoderLayer: 3-4                      [8, 512, 768]             5,713,408\n",
              "│    │    └─LunaTransformerEncoderLayer: 3-5                      [8, 512, 768]             5,713,408\n",
              "│    │    └─LunaTransformerEncoderLayer: 3-6                      [8, 512, 768]             5,713,408\n",
              "│    │    └─LunaTransformerEncoderLayer: 3-7                      [8, 512, 768]             5,713,408\n",
              "│    │    └─LunaTransformerEncoderLayer: 3-8                      [8, 512, 768]             5,713,408\n",
              "│    │    └─LunaTransformerEncoderLayer: 3-9                      [8, 512, 768]             5,713,408\n",
              "│    │    └─LunaTransformerEncoderLayer: 3-10                     [8, 512, 768]             5,713,408\n",
              "│    │    └─LunaTransformerEncoderLayer: 3-11                     [8, 512, 768]             5,713,408\n",
              "│    │    └─LunaTransformerEncoderLayer: 3-12                     [8, 512, 768]             5,713,408\n",
              "├─Luna_TransformerDecoder: 1-2                                    [8, 128, 32500]           --\n",
              "│    └─Embedding: 2-5                                             [8, 128, 768]             24,960,000\n",
              "│    └─ModuleList: 2-6                                            --                        --\n",
              "│    │    └─LunaTransformerDecoderLayer: 3-13                     [8, 128, 768]             9,256,960\n",
              "│    │    └─LunaTransformerDecoderLayer: 3-14                     [8, 128, 768]             9,256,960\n",
              "│    │    └─LunaTransformerDecoderLayer: 3-15                     [8, 128, 768]             9,256,960\n",
              "│    │    └─LunaTransformerDecoderLayer: 3-16                     [8, 128, 768]             9,256,960\n",
              "│    │    └─LunaTransformerDecoderLayer: 3-17                     [8, 128, 768]             9,256,960\n",
              "│    │    └─LunaTransformerDecoderLayer: 3-18                     [8, 128, 768]             9,256,960\n",
              "│    │    └─LunaTransformerDecoderLayer: 3-19                     [8, 128, 768]             9,256,960\n",
              "│    │    └─LunaTransformerDecoderLayer: 3-20                     [8, 128, 768]             9,256,960\n",
              "│    │    └─LunaTransformerDecoderLayer: 3-21                     [8, 128, 768]             9,256,960\n",
              "│    │    └─LunaTransformerDecoderLayer: 3-22                     [8, 128, 768]             9,256,960\n",
              "│    │    └─LunaTransformerDecoderLayer: 3-23                     [8, 128, 768]             9,256,960\n",
              "│    │    └─LunaTransformerDecoderLayer: 3-24                     [8, 128, 768]             9,256,960\n",
              "│    └─Linear: 2-7                                                [8, 128, 32500]           24,960,000\n",
              "===================================================================================================================\n",
              "Total params: 254,548,992\n",
              "Trainable params: 254,548,992\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (G): 6.46\n",
              "===================================================================================================================\n",
              "Input size (MB): 0.04\n",
              "Forward/backward pass size (MB): 4443.77\n",
              "Params size (MB): 1018.10\n",
              "Estimated Total Size (MB): 5461.91\n",
              "==================================================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iter_steps = 300"
      ],
      "metadata": {
        "id": "ylkl6gQ7rMUn"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(vanillar_transformer, inputs, decoder_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6f2fae2-932c-4df8-b3d1-6b804ec8d683",
        "id": "Ocz8BVm1rMUn"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [02:02<00:00,  2.46it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train(luna_transformer, inputs, decoder_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0aabc4e9-0f18-4395-8666-0c72626c4e5c",
        "id": "BCRv_1yWrMUo"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [02:32<00:00,  1.97it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P-s7sHz0uXqe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}